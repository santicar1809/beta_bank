{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripción del proyecto\n",
    "\n",
    "Los clientes de Beta Bank se están yendo, cada mes, poco a poco. Los banqueros descubrieron que es más barato salvar a los clientes existentes que atraer nuevos.\n",
    "\n",
    "Necesitamos predecir si un cliente dejará el banco pronto. Tú tienes los datos sobre el comportamiento pasado de los clientes y la terminación de contratos con el banco.\n",
    "\n",
    "Crea un modelo con el máximo valor F1 posible. Para aprobar la revisión, necesitas un valor F1 de al menos 0.59. Verifica F1 para el conjunto de prueba. \n",
    "\n",
    "Además, debes medir la métrica AUC-ROC y compararla con el valor F1.\n",
    "\n",
    "# Instrucciones del proyecto\n",
    "\n",
    "1. Descarga y prepara los datos.Explica el procedimiento.\n",
    "2. Examina el equilibrio de clases. Entrena el modelo sin tener en cuenta el desequilibrio. - Describe brevemente tus hallazgos.\n",
    "3. Mejora la calidad del modelo. Asegúrate de utilizar al menos dos enfoques para corregir el desequilibrio de clases. Utiliza conjuntos de entrenamiento y validación para encontrar el mejor modelo y el mejor conjunto de parámetros. Entrena diferentes modelos en los conjuntos de entrenamiento y validación. Encuentra el mejor. Describe brevemente tus hallazgos.\n",
    "4. Realiza la prueba final.\n",
    "\n",
    "# Descripción de los datos\n",
    "\n",
    "Puedes encontrar los datos en el archivo  /datasets/Churn.csv file. Descarga el conjunto de datos.\n",
    "\n",
    "## Características\n",
    "\n",
    "- RowNumber: índice de cadena de datos\n",
    "- CustomerId: identificador de cliente único\n",
    "- Surname: apellido\n",
    "- CreditScore: valor de crédito\n",
    "- Geography: país de residencia\n",
    "- Gender: sexo\n",
    "- Age: edad\n",
    "- Tenure: período durante el cual ha madurado el depósito a plazo fijo de un cliente (años)\n",
    "- Balance: saldo de la cuenta\n",
    "- NumOfProducts: número de productos bancarios utilizados por el cliente\n",
    "- IsActiveMember: actividad del cliente (1 - sí; 0 - no)\n",
    "- HasCrCard: el cliente tiene una tarjeta de crédito (1 - sí; 0 - no)\n",
    "- EstimatedSalary: salario estimado\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "- Exited: El cliente se ha ido (1 - sí; 0 - no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "from scipy.stats import randint\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    f1_score,\n",
    "    recall_score\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('datasets/Churn.csv')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para pasar columnas al formato snake_case\n",
    "def to_snake_case(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['row_number', 'customer_id', 'surname', 'credit_score', 'geography',\n",
      "       'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_cr_card',\n",
      "       'is_active_member', 'estimated_salary', 'exited'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Pasamos las columnas al modo snake_case\n",
    "columns=data.columns\n",
    "new_cols=[]\n",
    "for i in columns:\n",
    "    i=to_snake_case(i)\n",
    "    new_cols.append(i)\n",
    "data.columns=new_cols\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>9091.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569094e+07</td>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>4.997690</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.89568</td>\n",
       "      <td>7.193619e+04</td>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.894723</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.556570e+07</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2500.75000</td>\n",
       "      <td>1.562853e+07</td>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5000.50000</td>\n",
       "      <td>1.569074e+07</td>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7500.25000</td>\n",
       "      <td>1.575323e+07</td>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>1.581569e+07</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        row_number   customer_id  credit_score           age       tenure  \\\n",
       "count  10000.00000  1.000000e+04  10000.000000  10000.000000  9091.000000   \n",
       "mean    5000.50000  1.569094e+07    650.528800     38.921800     4.997690   \n",
       "std     2886.89568  7.193619e+04     96.653299     10.487806     2.894723   \n",
       "min        1.00000  1.556570e+07    350.000000     18.000000     0.000000   \n",
       "25%     2500.75000  1.562853e+07    584.000000     32.000000     2.000000   \n",
       "50%     5000.50000  1.569074e+07    652.000000     37.000000     5.000000   \n",
       "75%     7500.25000  1.575323e+07    718.000000     44.000000     7.000000   \n",
       "max    10000.00000  1.581569e+07    850.000000     92.000000    10.000000   \n",
       "\n",
       "             balance  num_of_products  has_cr_card  is_active_member  \\\n",
       "count   10000.000000     10000.000000  10000.00000      10000.000000   \n",
       "mean    76485.889288         1.530200      0.70550          0.515100   \n",
       "std     62397.405202         0.581654      0.45584          0.499797   \n",
       "min         0.000000         1.000000      0.00000          0.000000   \n",
       "25%         0.000000         1.000000      0.00000          0.000000   \n",
       "50%     97198.540000         1.000000      1.00000          1.000000   \n",
       "75%    127644.240000         2.000000      1.00000          1.000000   \n",
       "max    250898.090000         4.000000      1.00000          1.000000   \n",
       "\n",
       "       estimated_salary        exited  \n",
       "count      10000.000000  10000.000000  \n",
       "mean      100090.239881      0.203700  \n",
       "std        57510.492818      0.402769  \n",
       "min           11.580000      0.000000  \n",
       "25%        51002.110000      0.000000  \n",
       "50%       100193.915000      0.000000  \n",
       "75%       149388.247500      0.000000  \n",
       "max       199992.480000      1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   row_number        10000 non-null  object \n",
      " 1   customer_id       10000 non-null  object \n",
      " 2   surname           10000 non-null  object \n",
      " 3   credit_score      10000 non-null  int64  \n",
      " 4   geography         10000 non-null  object \n",
      " 5   gender            10000 non-null  object \n",
      " 6   age               10000 non-null  int64  \n",
      " 7   tenure            9091 non-null   float64\n",
      " 8   balance           10000 non-null  int32  \n",
      " 9   num_of_products   10000 non-null  int64  \n",
      " 10  has_cr_card       10000 non-null  int64  \n",
      " 11  is_active_member  10000 non-null  int64  \n",
      " 12  estimated_salary  10000 non-null  int32  \n",
      " 13  exited            10000 non-null  int64  \n",
      "dtypes: float64(1), int32(2), int64(6), object(5)\n",
      "memory usage: 1015.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#Cambiamos el tipo de dato a string\n",
    "data['row_number']=data['row_number'].astype(str)\n",
    "data['customer_id']=data['customer_id'].astype(str)\n",
    "data['balance']=data['balance'].astype(int)\n",
    "data['estimated_salary']=data['estimated_salary'].astype(int)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver a simple vista con el data info, que nuestros datos están bien, y que solo tenemos ausentes en la columna 'tenure', sin embargo, veremos más a fondo los ausetes y los duplicados.\n",
    "\n",
    "En cuanto a los tipos de datos vemos que las columnas 'RowNumber' y 'CustomerId' deberían ser tipo string para no cometer errores con estos números que no son significativos, sin embargo, debido a que el objetivo del proyecto es predecir los clientes que se van del banco, las tres primeras columnas no son relevantes para entrenar el modelo, por lo que no se tendrán en cuenta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausentes: \n",
      " row_number            0\n",
      "customer_id           0\n",
      "surname               0\n",
      "credit_score          0\n",
      "geography             0\n",
      "gender                0\n",
      "age                   0\n",
      "tenure              909\n",
      "balance               0\n",
      "num_of_products       0\n",
      "has_cr_card           0\n",
      "is_active_member      0\n",
      "estimated_salary      0\n",
      "exited                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Calculamos los ausentes\n",
    "print('Ausentes: \\n',data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de significancia: \n",
      " row_number          0.00\n",
      "customer_id         0.00\n",
      "surname             0.00\n",
      "credit_score        0.00\n",
      "geography           0.00\n",
      "gender              0.00\n",
      "age                 0.00\n",
      "tenure              9.09\n",
      "balance             0.00\n",
      "num_of_products     0.00\n",
      "has_cr_card         0.00\n",
      "is_active_member    0.00\n",
      "estimated_salary    0.00\n",
      "exited              0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Calculamos el porcentaje de significancia de los ausentes\n",
    "print('Porcentaje de significancia: \\n',100*data.isna().sum()/data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podeos ver que solo tenure tiene 909 ausentes el cual equivale al 9.09% de los datos, podemos trabajar con estos para imputarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>surname</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>num_of_products</th>\n",
       "      <th>has_cr_card</th>\n",
       "      <th>is_active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>15589475</td>\n",
       "      <td>Azikiwe</td>\n",
       "      <td>591</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140469</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>15766205</td>\n",
       "      <td>Yin</td>\n",
       "      <td>550</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103391</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>90878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>15768193</td>\n",
       "      <td>Trevisani</td>\n",
       "      <td>585</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146050</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>15702298</td>\n",
       "      <td>Parkhill</td>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125561</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>15651280</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>742</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136857</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9944</th>\n",
       "      <td>9945</td>\n",
       "      <td>15703923</td>\n",
       "      <td>Cameron</td>\n",
       "      <td>744</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>190409</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>138361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>9957</td>\n",
       "      <td>15707861</td>\n",
       "      <td>Nucci</td>\n",
       "      <td>520</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85216</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>117369</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9964</th>\n",
       "      <td>9965</td>\n",
       "      <td>15642785</td>\n",
       "      <td>Douglas</td>\n",
       "      <td>479</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117593</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>9986</td>\n",
       "      <td>15586914</td>\n",
       "      <td>Nepean</td>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123841</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130142</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_number customer_id    surname  credit_score geography  gender  age  \\\n",
       "30           31    15589475    Azikiwe           591     Spain  Female   39   \n",
       "48           49    15766205        Yin           550   Germany    Male   38   \n",
       "51           52    15768193  Trevisani           585   Germany    Male   36   \n",
       "53           54    15702298   Parkhill           655   Germany    Male   41   \n",
       "60           61    15651280     Hunter           742   Germany    Male   35   \n",
       "...         ...         ...        ...           ...       ...     ...  ...   \n",
       "9944       9945    15703923    Cameron           744   Germany    Male   41   \n",
       "9956       9957    15707861      Nucci           520    France  Female   46   \n",
       "9964       9965    15642785    Douglas           479    France    Male   34   \n",
       "9985       9986    15586914     Nepean           659    France    Male   36   \n",
       "9999      10000    15628319     Walker           792    France  Female   28   \n",
       "\n",
       "      tenure  balance  num_of_products  has_cr_card  is_active_member  \\\n",
       "30       NaN        0                3            1                 0   \n",
       "48       NaN   103391                1            0                 1   \n",
       "51       NaN   146050                2            0                 0   \n",
       "53       NaN   125561                1            0                 0   \n",
       "60       NaN   136857                1            0                 0   \n",
       "...      ...      ...              ...          ...               ...   \n",
       "9944     NaN   190409                2            1                 1   \n",
       "9956     NaN    85216                1            1                 0   \n",
       "9964     NaN   117593                2            0                 0   \n",
       "9985     NaN   123841                2            1                 0   \n",
       "9999     NaN   130142                1            1                 0   \n",
       "\n",
       "      estimated_salary  exited  \n",
       "30              140469       1  \n",
       "48               90878       0  \n",
       "51               86424       0  \n",
       "53              164040       1  \n",
       "60               84509       0  \n",
       "...                ...     ...  \n",
       "9944            138361       0  \n",
       "9956            117369       1  \n",
       "9964            113308       0  \n",
       "9985             96833       0  \n",
       "9999             38190       0  \n",
       "\n",
       "[909 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['tenure'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer=KNNImputer(n_neighbors=5)\n",
    "data[['credit_score','age','tenure','balance','num_of_products','estimated_salary']]=imputer.fit_transform(data[['credit_score','age','tenure','balance','num_of_products','estimated_salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_number          0.0\n",
      "customer_id         0.0\n",
      "surname             0.0\n",
      "credit_score        0.0\n",
      "geography           0.0\n",
      "gender              0.0\n",
      "age                 0.0\n",
      "tenure              0.0\n",
      "balance             0.0\n",
      "num_of_products     0.0\n",
      "has_cr_card         0.0\n",
      "is_active_member    0.0\n",
      "estimated_salary    0.0\n",
      "exited              0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Llenamos los ausentes con la mediana\n",
    "#data['tenure'].fillna(data['tenure'].median(),inplace=True)\n",
    "print(100*data.isna().sum()/data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llenamos los datos ausentes con la mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados: \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "print('Duplicados: \\n',data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tenemos duplicados, y rellenamos los ausentes, por lo cual estamos listos para comenzar el modelo de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examinamos el balanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos las frecuencias relativas de cada clase\n",
    "balance=data['exited'].value_counts(normalize=True)\n",
    "plt=balance.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que la clase \"0\" es predominante con un porcentaje aproximado del 80%, en cambio la clase \"1\" tiene solo el 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separamos el dataframe en entrenamiento y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=12345\n",
    "data_model=data.drop(['customer_id','row_number','surname'],axis=1)\n",
    "df_train,df_test=train_test_split(data_model,random_state=seed,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separamos el dataset en entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos los datos \n",
    "features= df_train.drop(['exited'],axis=1)\n",
    "target=df_train['exited']\n",
    "\n",
    "features_train, features_valid, target_train, target_valid=train_test_split(\n",
    "    features,target,random_state=seed,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalado de caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a escalar las características para que nuestro modelo pueda tomar estas variables\n",
    "numeric=['credit_score','age','tenure','balance','num_of_products','estimated_salary']\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric]=scaler.transform(features_train[numeric])\n",
    "features_valid[numeric]=scaler.transform(features_valid[numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación de las variables categoricas para los arboles\n",
    "\n",
    "Vamos a codificar con etiquetas el dataframe para los arboles y utilizaremos ONE-HOT para la regresion logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot=OneHotEncoder(drop='first')\n",
    "one_hot.fit(features_train[['geography','gender']])\n",
    "features_train[one_hot.get_feature_names_out()]=one_hot.transform(features_train[['geography','gender']]).todense()\n",
    "features_valid[one_hot.get_feature_names_out()]=one_hot.transform(features_valid[['geography','gender']]).todense()\n",
    "model=RandomForestClassifier(random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a pasar la lista de parametros que queremos iterar:\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Número de features a considerar para cada separación\n",
    "max_features = randint(1, 11)\n",
    "# Máximo número de niveles a considerar en el arbol\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Número mínimo  de pruebas requeridas para las eparación de un nodo\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Número minimo de pruebas requeridas para cada nodo hoja\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Metodo de selección de pruebas para el entrenamiento de cada árbol\n",
    "bootstrap = [True, False]\n",
    "# Creación de la malla aleatoria\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora junto con la malla y el RandomizedCV vamos a generar el mejor modelo con los mejores hiperparametros \n",
    "model_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=seed, n_jobs = -1)\n",
    "# Entrenamos el modelo\n",
    "model_random.fit(features_train,target_train)\n",
    "print(model_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud de nuestro modelo \n",
    "best_random = model_random.best_estimator_\n",
    "random_prediction = best_random.predict(features_valid)\n",
    "random_accuracy=metrics.accuracy_score(target_valid,random_prediction)\n",
    "recall_rf=metrics.recall_score(target_valid,random_prediction)\n",
    "f1_rf=metrics.f1_score(target_valid,random_prediction)\n",
    "roc_auc_rf=roc_auc_score(target_valid,random_prediction)\n",
    "print(\"Accuracy:\",random_accuracy)\n",
    "print('ROC-AUC: ',roc_auc_rf)\n",
    "print('Recall: ',recall_rf,'\\nF1-score: ',f1_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una buena exactitud aunque con datos desbalanceados, por lo que debemos balancearlos. Por otro lado, el area bajo la curva ROC nos indica que el modelo no es malo pero puede mejorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte con nuestras métricas de clasificación\n",
    "print(classification_report(target_valid,random_prediction))\n",
    "confusion_matrix = metrics.confusion_matrix(target_valid,random_prediction)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en la matriz que los datos están desbalanceados, debido a que el recall de la clase 1 es de 0.44 y el f1 es 0.55 lo que indica que del total de la suma de verdaderos positivos y falsos negativos, solo se está acertanco al 44% de los datos con categoría 1, por lo cual debemos hacer labores de balanceo para mejorar este resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros del arbol de decisión\n",
    "params = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "tree=DecisionTreeClassifier(random_state=seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iteramos para hallar los mejores hiperparametros para el arbol de decisión\n",
    "random_tree=RandomizedSearchCV(estimator = tree, param_distributions = params, n_iter = 100, cv = 3, verbose=2, random_state=seed, n_jobs = -1)\n",
    "# Entrenamos el modelo\n",
    "random_tree.fit(features_train,target_train)\n",
    "print(random_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_tree = random_tree.best_estimator_\n",
    "tree_prediction = best_tree.predict(features_valid)\n",
    "tree_accuracy=metrics.accuracy_score(target_valid,tree_prediction)\n",
    "roc_auc_dt=roc_auc_score(target_valid,tree_prediction)\n",
    "print(\"Accuracy:\",tree_accuracy)\n",
    "print('ROC-AUC: ',roc_auc_dt)\n",
    "recall_dt=metrics.recall_score(target_valid,tree_prediction)\n",
    "f1_dt=metrics.f1_score(target_valid,tree_prediction)\n",
    "print('Recall: ',recall_dt,'\\nF1-score: ',f1_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una buena exactitud aunque con datos desbalanceados al igual que el Random Forest, por lo que debemos balancearlos. Por otro lado, el area bajo la curva ROC da 0.62, lo que nos indica que el modelo no es malo pero puede mejorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_valid,tree_prediction))\n",
    "confusion_matrix = metrics.confusion_matrix(target_valid,tree_prediction)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que el bosque aleatorio, podemos ver que el desbalanceo afecta el recall y el f1 score de la clase 1, con valores de 0.28 y 0.41 respectivamente, incluso en este caso la precisión se vio afectada al bajar a un 62%. También debemos trabajar con estos datos para contraarrestar el balanceo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parametros de iteración de la regresión logística\n",
    "params={\n",
    "    'max_iter': range(100, 500),\n",
    "    'solver' : ['lbfgs', 'newton-cg', 'liblinear'],\n",
    "    'warm_start' : [True, False],\n",
    "    'C': np.arange(0.01, 1, 0.01)\n",
    "}\n",
    "log_reg=LogisticRegression(random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos el modelo\n",
    "log_random=RandomizedSearchCV(estimator=log_reg,param_distributions=params,n_iter=100,cv = 3, verbose=2, random_state=seed, n_jobs = -1)\n",
    "log_random.fit(features_train,target_train)\n",
    "print(log_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_log = log_random.best_estimator_\n",
    "log_prediction = best_log.predict(features_valid)\n",
    "log_accuracy=metrics.accuracy_score(target_valid,log_prediction)\n",
    "roc_auc_lr=roc_auc_score(target_valid,log_prediction)\n",
    "print(\"Accuracy:\",log_accuracy)\n",
    "print('ROC-AUC: ',roc_auc_lr)\n",
    "recall_lr=metrics.recall_score(target_valid,log_prediction)\n",
    "f1_lr=metrics.f1_score(target_valid,log_prediction)\n",
    "print('Recall: ',recall_lr,'\\nF1-score: ',f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exactitud de este modelo es la más baja de los tres modelos, al igual que el área bajo la curva, sin embargo, podemos mejorar este valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_valid,log_prediction))\n",
    "confusion_matrix = metrics.confusion_matrix(target_valid,log_prediction)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al análizar los resultados de la regresión logística, podemos ver que también es afectada por el desbalanceo, al tener un recall para la clase 1 de 0.17 y un recall de 0.27, lo que indica que tenemos que trabajar con el desbalanceo para lograr subir estas métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejora del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la función para arreglar el sobremuestreo\n",
    "def upsample(features, target, repeat):\n",
    "    #Primero dividimos el conjunto de datos de entrenamiento en positivos y negativos \n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    #Posteriormente multiplicamos los datos de la clase que tiene menos datos, en este caso la clase 1 y unimos todos los datos\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    #Por último, mesclamos todos los datos con la función shuffle y devolvemos los datos desbalanceados\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=seed\n",
    "    )\n",
    "\n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_upsampled_train, target_upsampled_train = upsample(\n",
    "    features_train, target_train, 3\n",
    ")\n",
    "features_upsampled_valid, target_upsampled_valid = upsample(\n",
    "    features_valid, target_valid, 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos las frecuencias relativas de cada clase\n",
    "balance=target_upsampled_train.value_counts(normalize=True)\n",
    "plt=balance.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance=target_upsampled_valid.value_counts(normalize=True)\n",
    "plt=balance.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest con sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo de nuevo con sobremuestreo\n",
    "model_random.fit(features_upsampled_train,target_upsampled_train)\n",
    "print(model_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud de nuestro modelo \n",
    "best_random = model_random.best_estimator_\n",
    "random_prediction_balanced = best_random.predict(features_upsampled_valid)\n",
    "random_accuracy_balanced=metrics.accuracy_score(target_upsampled_valid,random_prediction_balanced)\n",
    "roc_auc_rfb=roc_auc_score(target_upsampled_valid,random_prediction_balanced)\n",
    "print(\"Accuracy:\",random_accuracy_balanced)\n",
    "print('ROC-AUC: ',roc_auc_rfb)\n",
    "print('Diferencia accuracy: ',random_accuracy_balanced-random_accuracy)\n",
    "print('Diferencia ROC-AUC: ',roc_auc_rfb-roc_auc_rf)\n",
    "recall_rfb=metrics.recall_score(target_upsampled_valid,random_prediction_balanced)\n",
    "f1_rfb=metrics.f1_score(target_upsampled_valid,random_prediction_balanced)\n",
    "print('Recall: ',recall_rfb,'\\nF1-score: ',f1_rfb)\n",
    "print('Diferencia Recall: ',recall_rfb-recall_rf)\n",
    "print('Diferencia F1: ',f1_rfb-f1_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que al sobreajustar los datos se disminuye la exactitud en un 11% sin embargo, la calidad del modelo aumento en un 0.3% como lo vemos en la métrica ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte con nuestras métricas de clasificación\n",
    "print(classification_report(target_upsampled_valid,random_prediction_balanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_upsampled_valid,random_prediction_balanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que al triplicar los datos de la clase uno, nuestro dataset se balanceo y obtuvimos un mejor recall y f1 score sobre esa clase, obteniendo 0.45 y 0.60 respectivamente, sacrificando la exactitud que ahora es del 74%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree con sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tree.fit(features_upsampled_train,target_upsampled_train)\n",
    "print(random_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_tree_balanced = random_tree.best_estimator_\n",
    "tree_prediction_balanced = best_tree_balanced.predict(features_upsampled_valid)\n",
    "tree_accuracy_balanced=metrics.accuracy_score(target_upsampled_valid,tree_prediction_balanced)\n",
    "roc_auc_dtb=roc_auc_score(target_upsampled_valid,tree_prediction_balanced)\n",
    "print(\"Accuracy:\",tree_accuracy_balanced)\n",
    "print('ROC-AUC: ',roc_auc_dtb)\n",
    "print('Diferencia accuracy: ',tree_accuracy_balanced-tree_accuracy)\n",
    "print('Diferencia ROC-AUC: ',roc_auc_dtb-roc_auc_dt)\n",
    "recall_dtb=metrics.recall_score(target_upsampled_valid,tree_prediction_balanced)\n",
    "f1_dtb=metrics.f1_score(target_upsampled_valid,tree_prediction_balanced)\n",
    "print('Recall: ',recall_dtb,'\\nF1-score: ',f1_dtb)\n",
    "print('Diferencia Recall: ',recall_dtb-recall_dt)\n",
    "print('Diferencia F1: ',f1_dtb-f1_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al sobreajustar el arbol de decisión, podemos ver que la exactitud disminuyó en un 12% y el ROC-AUC aumento un 5.5%, lo que nos indica que la calidad del modelo aumentó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_upsampled_valid,tree_prediction_balanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_upsampled_valid,tree_prediction_balanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a nuestras métricas de recall y f1 score, pasamos de un 0.28 y 0.41 respectivamente, a 0.50 y 0.59, lo que nos indica que el sobremuestreo de nuestros datos mejoró estas métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression con sobremuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_random.fit(features_upsampled_train,target_upsampled_train)\n",
    "print(log_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_log_balanced = log_random.best_estimator_\n",
    "log_prediction_balanced = best_log_balanced.predict(features_upsampled_valid)\n",
    "log_accuracy_balanced=metrics.accuracy_score(target_upsampled_valid,log_prediction_balanced)\n",
    "roc_auc_lrb=roc_auc_score(target_upsampled_valid,log_prediction_balanced)\n",
    "print(\"Accuracy:\",log_accuracy_balanced)\n",
    "print('ROC-AUC: ',roc_auc_lr)\n",
    "print('Diferencia accuracy: ',log_accuracy_balanced-log_accuracy)\n",
    "print('Diferencia ROC-AUC: ',roc_auc_lrb-roc_auc_lr)\n",
    "recall_lrb=metrics.recall_score(target_upsampled_valid,log_prediction_balanced)\n",
    "f1_lrb=metrics.f1_score(target_upsampled_valid,log_prediction_balanced)\n",
    "print('Recall: ',recall_lrb,'\\nF1-score: ',f1_lrb)\n",
    "print('Diferencia Recall: ',recall_lrb-recall_lr)\n",
    "print('Diferencia F1: ',f1_lrb-f1_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ver el cambio de la exactitud, podemos ver que el sobreajuste disminuye en un 11%, sin embargo, la calidad de nuestro modelo aumento con la ROC-AUC en un 11%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_upsampled_valid,log_prediction_balanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_upsampled_valid,log_prediction_balanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ver nuestras otras métricas, el sobre ajuste cambió el recall y el f1 score respectivamente de 0.17 y 0.27 a 0.58 y 0.62, lo que es un cambio gigantezco en mejora de éstas métricas, lo que nos indica que el sobreajuste mejoro mucho muestro modelo de regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos la función para reducir el tamaño de la clase predominante y submuestrear los datos\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)]\n",
    "        + [features_ones]\n",
    "    )\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)]\n",
    "        + [target_ones]\n",
    "    )\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345\n",
    "    )\n",
    "\n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_downsampled_train, target_downsampled_train = downsample(\n",
    "    features_upsampled_train, target_upsampled_train, 0.75\n",
    ")\n",
    "features_downsampled_valid, target_downsampled_valid = downsample(\n",
    "    features_upsampled_valid, target_upsampled_valid, 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos las frecuencias relativas de cada clase\n",
    "balance=target_downsampled_train.value_counts(normalize=True)\n",
    "plt=balance.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance=target_downsampled_valid.value_counts(normalize=True)\n",
    "plt=balance.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest con submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el modelo de nuevo con sobremuestreo\n",
    "model_random.fit(features_downsampled_train,target_downsampled_train)\n",
    "print(model_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud de nuestro modelo \n",
    "best_random = model_random.best_estimator_\n",
    "random_prediction_subbalanced = best_random.predict(features_downsampled_valid)\n",
    "random_accuracy_subbalanced=metrics.accuracy_score(target_downsampled_valid,random_prediction_subbalanced)\n",
    "roc_auc_rfs=roc_auc_score(target_downsampled_valid,random_prediction_subbalanced)\n",
    "print(\"Accuracy:\",random_accuracy_subbalanced)\n",
    "print('ROC-AUC: ',roc_auc_rfs)\n",
    "print('Diferencia accuracy: ',random_accuracy_subbalanced-random_accuracy_balanced)\n",
    "print('Diferencia ROC-AUC: ',roc_auc_rfs-roc_auc_rfb)\n",
    "recall_rfs=metrics.recall_score(target_downsampled_valid,random_prediction_subbalanced)\n",
    "f1_rfs=metrics.f1_score(target_downsampled_valid,random_prediction_subbalanced)\n",
    "print('Recall: ',recall_rfs,'\\nF1-score: ',f1_rfs)\n",
    "print('Diferencia Recall: ',recall_rfs-recall_rfb)\n",
    "print('Diferencia F1: ',f1_rfs-f1_rfb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exactitud del modelo submuestreado al igual que en el sobremuestreo disminuye, teniendo en cuenta el modelos sobremuestreado, nuestra exactitud disminuye de nuevo en un 0.09%, sin embargo, nuestra curva ROC-AUC aumenta su calidad en un 2.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte con nuestras métricas de clasificación\n",
    "print(classification_report(target_downsampled_valid,random_prediction_subbalanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_downsampled_valid,random_prediction_subbalanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, aumentamos nuestras métricas de recall y f1 score, de 0.48, 0.62 a 0.52 y 0.66, lo que nos indica que nuestro submuestreo también mejoró estas métricas con respecto a nuestro dataset sobremuestreado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree con submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tree.fit(features_downsampled_train,target_downsampled_train)\n",
    "print(random_tree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_tree_subbalanced = random_tree.best_estimator_\n",
    "tree_prediction_subbalanced = best_tree_subbalanced.predict(features_downsampled_valid)\n",
    "tree_accuracy_subbalanced=metrics.accuracy_score(target_downsampled_valid,tree_prediction_subbalanced)\n",
    "roc_auc_dts=roc_auc_score(target_downsampled_valid,tree_prediction_subbalanced)\n",
    "print(\"Accuracy:\",tree_accuracy_subbalanced)\n",
    "print('ROC-AUC: ',roc_auc_dts)\n",
    "print('Diferencia accuracy: ',tree_accuracy_subbalanced-tree_accuracy_balanced)\n",
    "print('Diferencia ROC-AUC: ',roc_auc_dts-roc_auc_dtb)\n",
    "recall_dts=metrics.recall_score(target_downsampled_valid,tree_prediction_subbalanced)\n",
    "f1_dts=metrics.f1_score(target_downsampled_valid,tree_prediction_subbalanced)\n",
    "print('Recall: ',recall_dts,'\\nF1-score: ',f1_dts)\n",
    "print('Diferencia Recall: ',recall_dts-recall_dtb)\n",
    "print('Diferencia F1: ',f1_dts-f1_dtb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La exactitud aumenta en este caso un porcentaje muy mínimo, y nuestra calidad del modelo aumenta en un 2.7% la curva ROC-AUC, lo que indica que el submuestreo mejoró nuestro modelo con el dataset ya sobremuestreado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_downsampled_valid,tree_prediction_subbalanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_downsampled_valid,tree_prediction_subbalanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuesto submuestreo, aumento el recall y el f1 score de nuestro modelo de 0.50 y 0.59 a 0.56 y 0.65, lo que mejora la calidad de nuestro arbol de decición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression con submuestreo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_random.fit(features_downsampled_train,target_downsampled_train)\n",
    "print(log_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_log_subbalanced = log_random.best_estimator_\n",
    "log_prediction_subbalanced = best_log_subbalanced.predict(features_downsampled_valid)\n",
    "log_accuracy_subbalanced=metrics.accuracy_score(target_downsampled_valid,log_prediction_subbalanced)\n",
    "roc_auc_lrs=roc_auc_score(target_downsampled_valid,log_prediction_subbalanced)\n",
    "print(\"Accuracy:\",log_accuracy_subbalanced)\n",
    "print('ROC-AUC: ',roc_auc_lrs)\n",
    "print('Diferencia accuracy: ',log_accuracy_subbalanced-log_accuracy_balanced)\n",
    "print('Diferencia ROC-AUC: ',roc_auc_lrs-roc_auc_lrb)\n",
    "recall_lrs=metrics.recall_score(target_downsampled_valid,log_prediction_subbalanced)\n",
    "f1_lrs=metrics.f1_score(target_downsampled_valid,log_prediction_subbalanced)\n",
    "print('Recall: ',recall_lrs,'\\nF1-score: ',f1_lrs)\n",
    "print('Diferencia Recall: ',recall_lrs-recall_lrb)\n",
    "print('Diferencia F1: ',f1_lrs-f1_lrb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la exactitud disminuye un porcentaje muy mínimo, casi descartable, nuestra ROC-AUC, aumenta en un 1.1%, lo que nos muestra que el submuestreo mejoró nuestro modelo ligeramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_downsampled_valid,log_prediction_subbalanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_downsampled_valid,log_prediction_subbalanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recall y el f1 score aumentaron de 0.58 y 0.62 respectivamente a 0.69 y 0.69, lo que nos muestra que el submuestreo mejoró significativamente nuestra regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test= df_test.drop(['exited'],axis=1)\n",
    "target_test=df_test['exited']\n",
    "features_test[numeric]=scaler.transform(features_test[numeric])\n",
    "features_test[one_hot.get_feature_names_out()]=one_hot.transform(features_test[['geography','gender']]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_upsampled_test, target_upsampled_test = upsample(\n",
    "    features_test, target_test, 3\n",
    ")\n",
    "features_downsampled_test, target_downsampled_test = downsample(\n",
    "    features_upsampled_test, target_upsampled_test, 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos las frecuencias relativas de cada clase\n",
    "balance=target_downsampled_test.value_counts(normalize=True)\n",
    "plt=balance.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gráficamos las frecuencias relativas de cada clase\n",
    "balance=target_downsampled_test.value_counts(normalize=True)\n",
    "plt=balance.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest prueba final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud de nuestro modelo \n",
    "best_random = model_random.best_estimator_\n",
    "random_prediction_finalbalanced = best_random.predict(features_downsampled_test)\n",
    "random_accuracy_finalbalanced=metrics.accuracy_score(target_downsampled_test,random_prediction_finalbalanced)\n",
    "print(\"Accuracy:\",random_accuracy_finalbalanced)\n",
    "print('ROC-AUC: ',roc_auc_score(target_downsampled_test,random_prediction_finalbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte con nuestras métricas de clasificación\n",
    "print(classification_report(target_downsampled_test,random_prediction_finalbalanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_downsampled_test,random_prediction_finalbalanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al probar el mejor random forest y balanceando los datos del grupo test con nuestras dos técnicas de sobremuestreo y submuestreo, obtenemos lo siguiente: La exactitud del modelo fue del 72% y la curva ROC-AUC del 73%, Adicionalmente nuestro recall fue de 0.53 y el f1 score de 0.66. \n",
    "\n",
    "Si hacemos un recuento de estos resultados, el modelo pasa con una exactitud un poco baja debido a las técnicas de balanceo, sin embargo, tiene una mejor calidad que el dataset desbalanceado que era de ROC-AUC 70%. Esto quiere decir que en general nuestro modelo mejoró al aplicar las técnicas de balanceo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree prueba final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_tree_finalbalanced = random_tree.best_estimator_\n",
    "tree_prediction_finalbalanced = best_tree_finalbalanced.predict(features_downsampled_test)\n",
    "tree_accuracy_finalbalanced=metrics.accuracy_score(target_downsampled_test,tree_prediction_finalbalanced)\n",
    "print(\"Accuracy:\",tree_accuracy_finalbalanced)\n",
    "print('ROC-AUC: ',roc_auc_score(target_downsampled_test,tree_prediction_finalbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_downsampled_test,tree_prediction_finalbalanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_downsampled_test,tree_prediction_finalbalanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al probar el arbol de decisión y balanceando los datos del grupo test con nuestras dos técnicas de sobremuestreo y submuestreo, obtenemos lo siguiente: La exactitud del modelo fue del 67% y la curva ROC-AUC del 67%, Adicionalmente nuestro recall fue de 0.48 y el f1 score de 0.60. \n",
    "\n",
    "Si hacemos un recuento de estos resultados, el modelo pasa con una exactitud un poco baja debido a las técnicas de balanceo, además bajó la calidad con respecto al dataset desbalanceado que era de ROC-AUC 70%. Esto quiere decir que en general nuestro modelo empeoró al aplicar las técnicas de balanceo, quizas porque el modelo es muy sensible al modificar los datos con las técnicas de balanceo. De todas maneras se cumplió dejando el f1 score alto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression prueba final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Medimos la exactitud del modelo\n",
    "best_log_finalbalanced = log_random.best_estimator_\n",
    "log_prediction_finalbalanced = best_log_finalbalanced.predict(features_downsampled_test)\n",
    "log_accuracy_finalbalanced=metrics.accuracy_score(target_downsampled_test,log_prediction_finalbalanced)\n",
    "print(\"Accuracy:\",log_accuracy_finalbalanced)\n",
    "print('ROC-AUC: ',roc_auc_score(target_downsampled_test,log_prediction_finalbalanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mostramos la matriz de confusión y el reporte de métricas de clasificación\n",
    "print(classification_report(target_downsampled_test,log_prediction_finalbalanced))\n",
    "confusion_matrix = metrics.confusion_matrix(target_downsampled_test,log_prediction_finalbalanced)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "\n",
    "plt=cm_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, la regresión logística y balanceando los datos del grupo test con nuestras dos técnicas de sobremuestreo y submuestreo, 0.70 y el f1 score de 0.71. \n",
    "\n",
    "Si hacemos un recuento de estos resultados, el modelo pasa con una exactitud un poco baja debido a las técnicas de balanceo, además bajó la calidad con respecto al dataset desbalanceado que era de ROC-AUC 57%. Esto quiere decir que este modelo fue el más beneficiado por el balanceo, debido a que su calidad aumento mucho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "1. El balanceo mejora la calidad de los modelos de clasificación teniendo en cuenta la curva ROC-AUC, su f1-score y su recall, sin embargo, al aplicar las técicas de sobremuestreo y submuestreo, la exactitud de nuestros modelos baja significativamente.\n",
    "\n",
    "2. El mejor modelo evaluado fue el random forest que tuvo una métrica de ROC-AUC del 72%, seguido de la regresión logística con 70% y el arbol de decisión con un 69%.\n",
    "\n",
    "3. El arbol de decisión fue sensible a las técnicas de balanceo, por lo tanto, se vio afectada su métrica ROC-AUC y empeoró con respecto al dataset sin balancear, aunque sus métricas de recall y f1-score aumentaron."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
